@InProceedings{2017_EA_70,
author    = {Xambó, Anna and Roma, Gerard},
title     = {Hyperconnected Action Painting},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {0--1},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This performance invites the audience to participate in an immersive experience using their mobile devices. The aim is at capturing their actions on a digital painting inspired by Jackson Pollock's action painting technique. The audience is connected to a wireless network and a Web Audio application that recognizes a number of gestures through the mobile accelerometer sensor, which trigger different sounds. Gestures will be recognized and mapped to a digital canvas. A set of loudspeakers will complement the audience's actions with ambient sounds. The performance explores audio spatialization using both loudspeakers and mobile phone speakers, that combined with the digital painting provides an immersive audiovisual experience. The final digital canvas will be available online as a memory of the performance.},
doi       = {10.1093/gao/9781884446054.article.t000382},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/- 2017 - Hyperconnected Action Painting.pdf:pdf},
type      = {Performance},
}

@InProceedings{2017_EA_60,
author    = {Letz, Stéphane and Orlarey, Yann and Fober, Dominique},
title     = {Compiling Faust audio DSP code to WebAssembly},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {After a first version based on asm.js [4], we show in this paper how the Faust audio DSP language can be used to generate ecient Web Audio nodes based on WebAssembly. Two new compiler backends have been developed. The libfaust library version of the compiler has been compiled for the Web, thus allowing to have an ecient compilation chain from Faust DSP sources and libraries to audio nodes directly available in the browser.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Letz, Orlarey, Fober - 2017 - Compiling Faust audio DSP code to WebAssembly.pdf:pdf},
keywords  = {audio,domain specific,dsp,faust,language,or musical languages,real-time,various javascript dsp libraries,web audio api,webassembly},
type      = {Poster},
}

@InProceedings{2017_EA_55,
author    = {Houge, Ben},
title     = {Web Audio in the Dining Room},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {The worlds of the arts, gastronomy, and technology have been colliding and recombining with increasing frequency in recent years, resulting in unusual new forms of expression that engage with all of the senses in a uniquely responsive way. Artists are increasingly seeking to exploit the expressive potential of the chemical senses of smell and taste, while at the same time chefs have been investigating how to refine the narrative, aesthetic, and communicative capabilities of a meal [7]. Web Audio technologies offer a uniquely practicable solution to some of the unexpected challenges that arise when developing multisensory dining experiences, especially when considering the inherently indeterminate nature of a meal. A sophisticated and responsive real-time system is required to respond to diners' unpredictable choices and actions. Siting the source of the sound as close as possible to the food helps reinforce the links between the perception of taste, smell, and sound. Presenting the soundtrack to a meal via the mobile devices that diners are already bringing with them to the restaurant setting provides an innovative, scalable, and cost-effective solution to these challenges. Moreover, by coordinating the music emanating from each diner's device, the restaurant space is transformed into an emergent sound environment, driven by the activities of the diners.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Houge - 2017 - Web Audio in the Dining Room.pdf:pdf},
type      = {Talk},
url       = {https://youtu.be/OpUeyRRPpCo?t=5899},
}

@Misc{2017_KN2,
author    = {Schroeder, Franziska},
title     = {Keynote #2 Distributed Listening - A Practitioner's Perspective},
month     = aug,
year      = {2017},
abstract  = {The sonorous network has changed how we make and listen to music in many exciting but also challenging ways. Whereas traditional performance spaces exude notions of unity, togetherness, coherence and situatedness, the network has challenged performers to listen closely to the superimposition of acoustics, while being confronted with the socially dynamic and the often musically unknown. As performers we find ourselves in this almost schizophrenic state of being in a distant and sonically less identifiable space (something we grasp better with our ears), while also occupying a rather intimate and embodied listening space (something we grasp with our ears but also through touch). As a performer I will draw on a few musical performance scenarios to explore this sonic flânerie, a musicking where our ear is urged to reach across nodes, and which ultimately positions listening as a corporeal and multi-dimensional experience that is continuously being re-shaped by technological, socio-political and cultural concerns.},
address   = {London},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Thalmann, Florian and Ewert, Sebastian},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/e53854ebaa43e806b276a57d6a30ddd7926b55f2.html:html},
publisher = {Queen Mary University of London},
series    = {WAC '17},
type      = {Keynote},
url       = {https://www.youtube.com/watch?v=ZdxM4sHpKXg},
}

@InProceedings{2017_EA_28,
author    = {Pedersen, Benjamin},
title     = {Hi-precision audio in listening tests - also in the browser?},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {In listening tests, detailed sound control is sometimes mandatory down to each individual digital sample value and guarantee is needed that they are not unintentionally altered. At other times, a lesser degree of control is acceptable, if on the other hand test execution becomes less restricted. Detailed control of sound is often possible only under “laboratory” conditions where hardware and software are under complete control and sound pressure levels can be accurately calibrated. On the other hand, if test persons can do listening tests at home, via an internet browser for example, collecting large amounts of data becomes faster and cheaper (no laboratory facilities required, and more persons can do tests in parallel). Online listening tests made possible by the Web Audio API offers great flexibility in test execution, but compromises in precise stimulus control must be accepted. This paper analyzes such compromises by discussing technological limitations of Web Audio API followed by validation measurements of sound playback in popular internet browsers. The measurements show that at the detailed level there are significant differences in actual performance of different browsers and behavior is not always as expected. Finally, a solution is presented where audio presentation is delegated to an external audio presenter for situations where the limitations of Web Audio API are not acceptable.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Pedersen - 2017 - Hi-precision audio in listening tests - also in the browser.pdf:pdf},
keywords  = {Browser,online audio testing},
type      = {Poster},
}

@InProceedings{2017_EA_50,
author    = {Ziya, Ehsan},
title     = {Apophenia},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2017},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Apophenia is an interactive music piece that sets out to explore ways to achieve a balance between artistic narration and interactivity in a generative music system. The piece is an exploration in presenting interactive musical systems with an element of narration. The piece is delivered as an interactive musical system that heavily utilises the Web Audio API for real-time sound synthesis, sampling and audio processing. Users are initially presented with a set of points where each points represents a note. The user can use the mouse to hover over multiple notes to generate chords and trigger melodies by clicking. The piece progresses as the user finds special connections between the notes. These special connections will reveal a visual pattern and eventually the user will discover a new dimension in the piece. Eventually, the piece would go back to the beginning where the user can start from scratch. The visual aspect of the work starts with a 2D interactive system made using pt.js [1] library. The second phase of the experience is in 3D which utilises WebGL through Three.js [2].},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Ziya - 2017 - Apophenia.pdf:pdf},
type      = {Artwork},
}

@InProceedings{2017_EA_84,
author    = {Milde, Jan-Torsten},
title     = {Synchronized mobile devices using web audio technology on a Raspberry Pi},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This paper describes the ongoing development of a system for the creation of a distributed musical space: the MusicBox. The MusicBox has been realized as an open access point for mobile devices. It provides a musical web application enabling the musician to distribute audio events onto the connected mobile devices and control synchronous playback of these events. In order to locate the mobile devices, a microphone array has been developed, allowing to automatically identify sound direction of the connected mobile devices. This makes it possible to control the position of audio events in the musical space. The system has been implemented on a Raspberry Pi, making it very cheap and robust. No network access is needed to run the MusicBox, turning it into a versatile tool to setup interactive distributed music installations.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Milde - 2017 - Synchronized mobile devices using web audio technology on a Raspberry Pi.pdf:pdf},
type      = {Poster},
}

@InProceedings{2017_EA_32,
author    = {Mckegg, Matt},
title     = {Loop Drop: Live Electronic Music Software powered by Web Audio (demo)},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2017},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {As a follow up to my talk "Is Web Audio ready for the stage / dancefloor" I also submit a demo of the software Loop Drop (http:// loopjs.com) that I will be mostly talking about. I'll also be available to discuss with attendees in more depth about the various associated challenges and the endless possibilities opened up by these new web standards!},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mckegg - 2017 - Loop Drop Live Electronic Music Software powered by Web Audio ( demo ).pdf:pdf},
type      = {Demo},
}

@InProceedings{2017_EA_22,
author    = {Guttandin, Christoph},
title     = {Noiseless Web Audio Tests},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2017},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {There is a reason why the sentence "bring your own headphones" is part of every invitation to an audio related hackathon. And of course the whole point of working with audio engines like the Web Audio API is to create impressive sonic experiences. But while building such an experience it can quickly become cumbersome to test the same sound over and over again. A hackathon project might not need any tests at all, but more mature projects will benefit from tests which can be run automatically. This is even more important for projects which deal with ever evolving APIs like the Web Audio API. Every browser update may introduce a breaking change. If you don't have automated tests to check if everything still works as expected, you might need to keep your headphones at hand. I would like to introduce a new category of tests which I call expectation tests. The idea behind expectation tests is to separate code which deals with browser anomalies from the business logic. This opens up the possibility to write code as if you dealt with one single (perfect) browser. Whenever there is a shortcoming in a supported browser which needs to be patched with a polyfill, this should be accompanied by an expectation test which checks if the browser still behaves wrongly. Later on this test will break when the polyfill can be removed again. This makes sure that only necessary patches are part of the production code. In addition to that, the polyfills should be completely separate from any business logic. Instead the polyfills should make sure to provide a uniform API in each supported browser, which then can be used by the business logic. Another benefit from using expectation tests is that it's not necessary anymore to test your business logic with the "real" Web Audio API. I would like to present a framework which mocks the Web Audio API and allows to advance the currentTime of an AudioContext in a controlled way to make sure scheduling and AudioParam automations work correctly.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Guttandin - 2017 - Noiseless Web Audio Tests.pdf:pdf},
type      = {Talk},
url       = {https://youtu.be/HjBqB3g8y2A?t=2627},
}

@InProceedings{2017_EA_75,
author    = {Roberts, Charlie},
title     = {Frabjous day},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2017},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Frabjous day is a live-coding performance using a new, browser-based live-coding environment, gibberwocky, co-developed by the performer with Dr. Graham Wakefi e ld. gibberwocky fea tures deepintegration with Ableton Live, possessing a variety of affordances for both musical sequencing and rapidly creating / assigning audio-rate modulation graphs. Like Gibber, another browser-based envi ronment developed by the performer, gibberwocky places emphasis on dynamic annotations to source code that reveal the state of underlying algorithms. A new addition found only in gibberwocky isthe use of animated sparklines to visually depict synthesis modulations over time. gibberwocky also makes heavy use of a new JavaScript synthesis library, genish.js, to generate musical patterns via digital signal processing techniques.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Roberts - 2017 - Frabjous day.pdf:pdf},
type      = {Performance},
}

@Misc{2017_KN1,
author    = {Chafe, Chris},
title     = {Keynote #1 Probing Rhythmic Synchronization in out Mind's Ear},
month     = aug,
year      = {2017},
abstract  = {Web Audio enables crowd-sourced surveys involving sound synthesis and interaction. My music-making experiences with collaboration technologies like Jacktrip have led to studies of how we imagine flows of sound in time, particularly those which require tight synchronization. Web-based “active listening” techniques make it possible to gather self-reports from large numbers of individuals asking about how we hear, imagine and produce musical “objects” which unfold in time.},
address   = {London},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Thalmann, Florian and Ewert, Sebastian},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/4108ba1d2c05877154e9deed3031b6e7de56709b.html:html},
publisher = {Queen Mary University of London},
series    = {WAC '17},
type      = {Keynote},
url       = {https://www.youtube.com/watch?v=BhL3J5hcwNE&t=960},
}

@InProceedings{2017_EA_14,
author    = {Wallace, Tony},
title     = {WebX0X Version 2},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {0--1},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {WebX0X Version 2 is a performance-oriented drum synthesizer and sequencer built using the Web Audio API.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Wallace - 2017 - WebX0X Version 2.pdf:pdf},
type      = {Demo},
}

@InProceedings{2017_EA_69,
author    = {Ball, Edward},
title     = {Sonification and Visualization of Parametric Equations},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {3--4},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This paper describes how parametric equations can be sonified using the Web Audio API and visualized using a vectorscope built using the HTML5 canvas element. A working demonstration can be found at academo.org/demos/vectorscope, with a selection of user-adjustable parametric equations, including Lissajous figures and hypotrochoids.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Ball - 2017 - Sonification and Visualization of Parametric Equations.pdf:pdf},
type      = {Artwork},
}

@InProceedings{2017_26,
author    = {Buffa, Michel and Lebrun, Jérôme},
title     = {Real time tube guitar amplifier simulation using WebAudio},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {1--9},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This paper presents a tube guitar amplifier simulation made with the WebAudio API, that reproduces the main parts of the Marshall JCM 800 amplifier schematics. Each stage of the real amp has been recreated (preamp, tone stack, reverb, power amp and speaker simulation, and we added an extra multiband EQ). The “classic rock” amp simulation we built has been used in real gigs and can be compared with some native amp simulation both in terms of latency, sound quality, dynamics and comfort of the guitar play. Unfortunately, as of today, low latency can be achieved only with certain configurations, due to audio driver limitations of current browsers on certain operating systems. The paper discusses the latency problems encountered with WebAudio, common traps, current limitations, and proposes some solutions. The final web based simulation has been compared with native re- creations of the same amp model (including commercial products such as GuitarRig, the JCM800 amp included in GarageBand or the open source Guitarix amp sim that runs on Linux), and with a real amp: the Yamaha THR10 that comes with a model of a Marshall amp. We conducted both quantitative evaluations (measure of the “guitar-to-speaker” latency, group delay, frequency response analysis) and qualitative evaluations with real guitar players who compared, guitar in hands, the different simulations in terms of sound quality and dynamics, and more generally “how they feel playing guitar with these simulations”. The amp is open source1 and can be tested online2, even without a guitar (it comes with an audio player, dry guitar samples and a wave generator that can be used at input). The Web page contains links to the source code repository, tutorial videos and a complete report of the measures we made, with different configurations (various soundcard, operating system, browsers), that is summarized in this paper. Figure 1 shows the current GUI (with optional frequency analyzers and oscilloscopes we used to probe the signal at different stages of the simulation). Our initial goal was to evaluate the limits of the WebAudio API and see if it was possible to design a web based guitar amp simulator that could compete with native simulations.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Buffa, Lebrun - 2017 - Real time tube guitar amplifier simulation using WebAudio.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/BhL3J5hcwNE?t=11246},
}

@InProceedings{2017_EA_34,
author    = {Schnell, Norbert and Matuszewski, Benjamin},
title     = {88 Fingers},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
volume    = {5},
series    = {WAC '17},
pages     = {4--5},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {88 Fingers is a performance in which up to 88 players in the audience perform on an automatized piano (i.e. a YAMAHA Disklavier) via their mobile devices. The piano is presented in the performance space as if it would be the instrument of a solo performer (e.g. on stage or in the center of the space). Apart from the web-based system that allows the participants to select a single key of the piano and to play it during the concert, the concept of the performance does not impose any further constraints. The performance is structured into two parts of 10 minutes separated by a discussion among the members of the audience of approximately 10 minutes. The experience establishes a metaphor of a free and responsible society.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Schnell, Matuszewski - 2017 - 88 Fingers.pdf:pdf},
keywords  = {Other},
type      = {Performance},
}

@InProceedings{2017_EA_45,
author    = {Donovan, Liam and Bin, S M Astrid and Armitage, Jack and Mcpherson, Andrew P},
title     = {Building an IDE for an Embedded System Using Web Technologies},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
number    = {August},
series    = {WAC '17},
pages     = {0--1},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Implementing an understandable, accessible and effective user interface is a major challenge for many products in the microcontroller and embedded computing community. Bela, an embedded system for ultra-low latency audio and sensor processing, features a browser-based integrated development environment (IDE) using web technologies (Node.js, HTML5 and CSS). This methodology has allowed us to create an IDE that is simplified and intuitive for beginners while still being useful to those more advanced, thus supporting users as they evolve in expertise.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Donovan et al. - 2017 - Building an IDE for an Embedded System Using Web Technologies.pdf:pdf},
type      = {Talk},
url       = {https://youtu.be/OpUeyRRPpCo?t=4791},
}

@InProceedings{2017_EA_52,
author    = {Su, David},
title     = {Runaway Reverie},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2017},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Runaway Reverie is a 3D platformer that also functions as an interactive song. Through the course of the dream-like game, players explore how their movement, location, and interactions with the environment affect musical parameters. This work was built using the Superpowers HTML5 engine using visual assets by Sparklin Labs, and extending the game engine's audio capabilities with Conductor and MultiSoundPlayer classes, which derive inspiration from layer and loop-driven interactive audio engines driven such as FMOD and Wwise. The implementation of these new functionalities demonstrates that such high-level game audio concepts can be applied to Web Audio as well. Finally, the artist hopes that the game will serve as a demonstration that interactive multimedia experiences, especially those that incorporate music and audio as an essential element, can make use of web technologies for ease of access.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Su - 2017 - Runaway Reverie.pdf:pdf},
type      = {Artwork},
}

@InProceedings{2017_40,
author    = {Meseguer-brocal, Gabriel and Peeters, Geoffroy and Pellerin, Guillaume and Buffa, Michel and Cabrio, Elena and Zucker, Catherine Faron and Giboin, Alain and Mirbel, Isabelle and Hennequin, Romain and Moussallam, Manuel and Piccoli, Francesco and Fillon, Thomas},
title     = {WASABI: a Two Million Song Database Project with Audio and Cultural Metadata plus WebAudio enhanced Client Applications},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This paper presents the WASABI project, started in 2017, which aims at (1) the construction of a 2 million song knowl-edge base that combines metadata collected from music databases on the Web, metadata resulting from the analysis of song lyrics, and metadata resulting from the audio analysis, and (2) the development of semantic applications with high added value to exploit this semantic database. A preliminary version of the WASABI database is already online 1 and will be enriched all along the project. The main originality of this project is the collaboration between the algorithms that will extract semantic metadata from the web and from song lyrics with the algorithms that will work on the audio. The following WebAudio enhanced applications will be associated with each song in the database: an online mixing table, guitar amp simulations with a virtual pedalboard, audio analysis visualization tools, annotation tools, a similarity search tool that works by uploading audio extracts or playing some melody using a MIDI device are planned as companions for the WASABI database.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Meseguer-brocal et al. - 2017 - WASABI a Two Million Song Database Project with Audio and Cultural Metadata plus WebAudio enhanced Clien.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/mo6VKewheGU?t=928},
}

@InProceedings{2017_EA_74,
author    = {Muzzulini, Daniel and Vogtenhuber, Raimund},
title     = {Sound Colour Space – A Virtual Museum*},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {4--7},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {By investigating the conceptual field of sound, tone, pitch, and timbre in its relation to visual phenomena and geometrical concepts, the project Sound Colour Space – A Virtual Museum contributes to an interdisciplinary field of research and explores its adequate modes of representation and communication. Many scientists and philosophers from antiquity to modern times have studied the relationships between sound, light and geometry. Many of their visualisations of acoustical, optical and perceptual topics speak to the eye and can be studied comparatively. These pictures are interesting because of their diagrammatic structure, in the way they combine text, images and spatial structures on a flat surface and in the way they address topological, philosophical and psychological questions. They often have an aesthetic value of their own. In addition to the development of an exemplary online publication, interactive audiovisual examples were created, which also were used for artistic projects.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Muzzulini, Vogtenhuber - 2017 - Sound Colour Space – A Virtual Museum.pdf:pdf},
type      = {Talk},
url       = {https://youtu.be/HjBqB3g8y2A?t=813},
}

@InProceedings{2017_18,
author    = {Zbyszyński, Michael and Grierson, Mick and Yee-king, Matthew and Fedden, Leon},
title     = {Write once run anywhere revisited: machine learning and audio tools in the browser with C++ and emscripten},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {1--5},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {A methodology for deploying interactive machine learning and audio tools written in C++ across a wide variety of platforms, including web browsers, is described. The work flow involves development of the code base in C++, making use of all the facilities available to C++ programmers, then transpiling to asm.js bytecode, using Emscripten to allow use of the libraries in web browsers. Audio capabilities are provided via the C++ Maximilian library that is transpiled and connected to the Web Audio API, via the ScriptProcessorNode. Machine learning is provided via the RapidLib library which implements neural networks, k-NN and Dynamic Time Warping for regression and classification tasks. An online, browser-based IDE is the final part of the system, making the toolkit available for education and rapid prototyping purposes, without requiring software other than a web browser. Two example use cases are described: rapid prototyping of novel, electronic instruments and education. Finally, an evaluation of the performance of the libraries is presented, showing that they perform acceptably well in the web browser, compared to the native counterparts but there is room for improvement here. The system is being used by thousands of students in our on-campus and online courses.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Zbyszy{\'{n}}ski et al. - 2017 - Write once run anywhere revisited machine learning and audio tools in the browser with C and emscripten.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/mo6VKewheGU?t=4643},
}

@InProceedings{2017_EA_73,
author    = {Zimmer, Sebastian},
title     = {An approach to assess loudness and dynamics with Web Audio native nodes},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Music content providers on the Internet like YouTube1, Spotify2 or Apple Music3, as well as a range of software playback systems like the media player “foobar2000”4 have a loudness normalization feature to match a series of diverse audio tracks in overall loudness. This is done to keep perceived volume differences between audio tracks as low as possible. Thus, it is important for music producers, especially mastering engineers, to master audio tracks with a particular amount of dynamic range, so that streaming services will not turn the playback volume of their tracks down. With their already low dynamic range, a listener would now even better be able to recognize their inferior sound compared to other tracks with higher dynamic range. To correctly assess the dynamics of audio material, this paper introduces two web applications that compute and visualize the loudness and dynamic range of audio material, using a subset of the loudness units described in the recommendation R 1285 by the European Broadcasting Union6, and using only native notes by the W3C Web Audio API7.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Zimmer - 2017 - An approach to assess loudness and dynamics with Web Audio native nodes.pdf:pdf},
keywords  = {dynamic range,ebu r 128,loudness,web audio api},
type      = {Talk},
url       = {https://youtu.be/HjBqB3g8y2A?t=1652},
}

@InProceedings{2017_EA_20,
author    = {Font, Frederic and Bandiera, Giuseppe},
title     = {Freesound Explorer: Make Music While Discovering Freesound!},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Freesound Explorer is a visual interface for exploring Freesound content in a two-dimensional space and creating music by linking content in that space. Freesound Explorer is implemented as a web application which takes advantage of modern web technologies including the Web Audio API and the Web MIDI API. This extended abstract describes Freesound Explorer's features and provides some technical details about its implementation.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Font, Bandiera - 2017 - Freesound Explorer Make Music While Discovering Freesound!.pdf:pdf},
type      = {Demo},
}

@InProceedings{2017_EA_43,
author    = {Fillon, Thomas and Pellerin, Guillaume},
title     = {A collaborative web platform for sound archives management and analysis},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {In the context of digital sound archives, an innovative web framework for automatic analysis and manual annotation of audio files has been developed. This web framework, is called Timeside and is available under an open-source license. The TimeSide framework associates an audio processing engine, an audio database, a web API and a client-side multi-media player. The audio processing engine is written in Python language and has been designed for speech and audio signal analysis and Music Information Retrieval (MIR) tasks. It includes a set of audio analysis plugins and additionally wraps several state-of-the-art audio features extraction libraries to provide automatic annotation, segmentation and Music Information Retrieval analysis. It also provides decoding and encoding methods for most common multimedia formats. The audio database application is handled through Django (Python) and is interfaced with the audio processing engine. The web API component provides these functionalities over the web to enable web client to run analysis on the sounds in the audio database. Last but not least, the multi-media player provides an web player associated with several sound and analysis visualizations together with an annotations editor through a multi-tracks display. The TimeSide platform is available as an open-source project at the following addresses: TimeSide: https://github.com/Parisson/TimeSide},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Fillon, Pellerin - 2017 - A collaborative web platform for sound archives management and analysis.pdf:pdf},
type      = {Demo},
}

@InProceedings{2017_EA_65,
author    = {Mitchusson, Chase and Marasco, Anthony T and Allison, Jesse},
title     = {Usage of Physics Engines for UI Design in NexusUI},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {1--2},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {In preparation to expand the experimental interfaces in NexusUI widgets, the authors have been evaluating physics engines and exploring physics-based user interfaces on the web. Tying physics simulation events, influenced by user interactions, to web audio encourages exploration of novel methods of interactivity between users and web-based instruments. Object collisions, deformation of a mesh of objects with elastic connections, and liquid simulation via particle generation were identified as systems with dynamics that may provide interesting links to audio synthesis. Two popular physics engines explored are LiquidFun and Matter.js, with new prototype widgets taking advantage of LiquidFun's Elastic Particles and Matter.js' Cloth and Newton's Cradle composites. One of our goals is to discover methods of audio synthesis that complement the behaviors of each physical simulation.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mitchusson, Marasco, Allison - 2017 - Usage of Physics Engines for UI Design in NexusUI.pdf:pdf},
type      = {Talk},
url       = {https://youtu.be/HjBqB3g8y2A?t=3422},
}

@InProceedings{2017_EA_38,
author    = {Jillings, Nicholas and Wang, Yonghao and Stables, Ryan and Reiss, Joshua D},
title     = {Intelligent audio plugin framework for the Web Audio API},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {The Web Audio API introduced native audio processing into web browsers. Audio plugin standards have been created for developers to create audio-rich processors and deploy them into media rich websites. It is critical these standards support flexible designs with clear host-plugin interaction to ease integration and avoid non-standard plugins. Intelligent features should be embedded into standards to help develop next-generation interfaces and designs. This paper presents a discussion on audio plugins in the web audio API, how they should behave and leverage web technologies with an overview of current standards.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Jillings et al. - 2017 - Intelligent audio plugin framework for the Web Audio API.pdf:pdf},
type      = {Talk},
url       = {https://youtu.be/OpUeyRRPpCo?t=2878},
}

@InProceedings{2017_EA_83,
author    = {Visser, Jeroen and Vogtenhuber, Raimund},
title     = {Cloudspeakers – a mobile performance network},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {4--5},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {In this project we developed a network of cloudspeakers. These are mobile speakers connected to a raspberry pi3 equipped with a low-latency audio card. They are connected to a wifi network and run a SuperCollider-Server (scsynth). Our cloudspeaker can be addressed with the SuperCollider (sclang) in the network. For this purpose we had to create a stable and scaleable network, and we had to find solutions for problems like latency, jitter, or software management. This network can be used for artistic projects and can be combined with a webserver and the use of mobile devices.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Visser, Vogtenhuber - 2017 - Cloudspeakers – a mobile performance network.pdf:pdf},
type      = {Poster},
}

@InProceedings{2017_EA_29,
author    = {Lind, Fredrik and MacPherson, Andrew},
title     = {Soundtrap: A collaborative music studio with Web Audio},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {3--4},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Soundtrap is an online music studio and DAW built using the latest Web Audio, Web MIDI and WebRTC standards and supported across nearly all major browsers. With over 1.5 million users and a focus on collaborative music production, the platform is used in both consumer and educational contexts around the world. Soundtrap aims to create an easy-to-use environment for users who are just getting started with music production, while still providing the advanced features required by more experienced users. The platform allows users to connect and compose music together using MIDI, live instruments and voice, in addition to providing a wide variety of loops and effects. Inside the studio users have access to integrated video chat over WebRTC, enabling instant feedback with collaborators as they create projects together in real-time. Soundtrap also works on a variety of hardware and platforms, scaling the audio experience to work on devices ranging from high end workstations to Chromebooks and mobile (Android and iOS). This scaling is made transparent to the user by auto-detecting basic performance characteristics on startup and during studio sessions, and modifying the Web Audio graph as necessary. The studio also uses a few additional techniques to work around device-specific challenges, such as freezing finished tracks within a project to ease the runtime CPU load, doing some processing server-side where possible, and using libvorbis through emscripten to encode large WAV files to ease memory requirements. As Soundtrap wants to match the functionality and performance of a native application DAW there are still some challenges encountered around issues like audio latency, streaming, disk usage, and greater access to multiple CPU cores. To this end Soundtrap developers are contributing code towards lower audio latencies in the Chromium browser and are closely following work on the Web Audio spec around things like AudioWorklets. There is also work being done on new projects around things like ”live jamming” with Web Audio over WebRTC, and integrations with other open-source projects like Google Magenta.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Lind, MacPherson - 2017 - Soundtrap A collaborative music studio with Web Audio.pdf:pdf},
keywords  = {Other},
type      = {Talk},
url       = {https://youtu.be/OpUeyRRPpCo?t=957},
}

@InProceedings{2017_51,
author    = {Werner, Nils and Balke, Stefan and Stöter, Fabian-robert and Müller, Meinard and Edler, Bernd},
title     = {trackswitch.js: A Versatile Web-Based Audio Player for Presenting Scientific Results},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {trackswitch.js is a versatile web-based audio player that enables researchers to conveniently present examples and results from scientific audio processing applications. Based on a multitrack architecture, trackswitch.js allows a listener to seamlessly switch between multiple audio tracks, while synchronously indicating the playback position within images associated to the audio tracks. These images may correspond to feature representations such as spectrograms or to visualizations of annotations such as structural boundaries or musical note information. The provided switching and playback functionalities are simple yet useful tools for analyzing, navigating, understanding, and evaluating results obtained from audio processing algorithms. Furthermore, trackswitch.js is an easily extendible and manageable software tool, designed for non-expert developers and unexperienced users. Offering a small but useful selection of options and buttons, trackswitch.js requires only basic knowledge to implement a versatile range of components for web-based audio demonstrators and user interfaces. Besides introducing the underlying techniques and the main functionalities of trackswitch.js we provide several use cases that indicate the flexibility and usability of our software for different audio-related research areas.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Werner et al. - 2017 - trackswitch.js A Versatile Web-Based Audio Player for Presenting Scientific Results.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/mo6VKewheGU?t=60},
}

@InProceedings{2017_80,
author    = {Allison, Jesse and Ostrenko, Frederick and Cellucci, Vincent},
title     = {Active Server Roles for Extended Distributed Performance Complexity in Diamonds in Dystopia},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {4--7},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Distributed performance systems that utilize a centralized server for connectivity have the potential to also provide extended computational and storage resources that would not be beneficial or even possible if distributed onto mobile clients. The usage of large datasets, shared or collaborative resources, and processor intensive techniques can be performed on the server while allowing time sensitive and less complex user specific computation to occur on client devices. Many of these types of problems can be solved using tools developed for cloud computing. This approach is demonstrated in the work Diamonds in Dystopia, a collaborative poetry performance that incorporates audience interaction on mobile devices, generation of poetic material on server side resources, real-time synthesis distributed through mobile and venue speakers, a live poetry reading performance and the live synthesis of poetry from the collective ensemble.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Allison, Ostrenko, Cellucci - 2017 - Active Server Roles for Extended Distributed Performance Complexity in Diamonds in Dystopia.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/BhL3J5hcwNE?t=8758},
}

@InProceedings{2017_EA_71,
author    = {Snyder, Jeff and Wallace, Drew},
title     = {Sunspots},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2017},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Sunspots is a web art piece that allows the audience to explore a virtual 3D world. It is an interactive component to a physical audio release on dual-LP vinyl (also called Sunspots), allowing the material to go beyond fixed-media representation. Three environments can be navigated, each with their own audio and visual textures. The visuals make use of Three.js and custom shaders to allow for strange cloth-based physical simulations live in the browser. WebAudio is used for creation and spatialization of the sound in the 3D environment. The audio for each environment is generated by overlapping multiple channels of "instruments" that are created by randomly loading different short segments of closely-related analog synthesizer material. These “instruments” are placed in different locations in the virtual world and cause audio-reactive visual behaviors through amplitude detection. Since the project is intended to accompany and compliment the physical audio recording, sample playback is used as opposed to direct WebAudio synthesis. The idea is to create generative versions of the pieces on the album that will vary endlessly and create a unique user experience.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Snyder, Wallace - 2017 - Sunspots.pdf:pdf},
type      = {Artwork},
}

@InProceedings{2017_24,
author    = {Kondak, Zachary and Liang, Alex and Tomlinson, Brianna and Walker, Bruce N},
title     = {Web Sonification Sandbox - an Easy-to-Use Web Application for Sonifying Data and Equations},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Auditory and multimodal presentation of data (“auditory graphs”) can allow for discoveries in a data set that are sometimes impossible with visual-only inspection. At the same time, multimodal graphs can make data, and the STEM fields that rely on them, more accessible to a much broader range of people, including many with disabilities. There have been a variety of software tools developed to turn data into sound, including the widely-used Sonification Sandbox, but there remains a need for simple, powerful, and more accessible tool for the construction and manipulation of multimodal graphs. Web-based audio functionality is now at the point where it can be leveraged to provide just such a tool. Thus, we developed a web application, the Web Sonification Sandbox (or simply the Web Sandbox), that allows users to create and manipulate multimodal graphs that convey information through both sonification and visualization. The Web Sandbox is designed to be usable by individuals with no technical or musical expertise, which separates it from existing software. The easy-to-use nature of the Web Sandbox, combined with its multimodal nature, allow it to be a maximally accessible application by a diverse audience of users. Nevertheless, the application is also powerful and flexible enough to support advanced users.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Kondak et al. - 2017 - Web Sonification Sandbox - an Easy-to-Use Web Application for Sonifying Data and Equations.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/BhL3J5hcwNE?t=7650},
}

@InProceedings{2017_EA_47,
author    = {Houge, Ben and Youssef, Jozef},
title     = {Quiver, Pop, and Dissolve: Three Essays in Gastromorphology},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {We propose a performance that uses web audio technologies to present a unique solution to the challenge of pairing music with the unpredictable choices and actions of diners in a restaurant. Three small dishes by a well-regarded London chef will be served, and audience members will experience a customized music pairing deployed via a web app running on their own mobile devices.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Houge, Youssef - 2017 - Quiver , Pop , and Dissolve Three Essays in Gastromorphology.pdf:pdf},
type      = {Performance},
}

@InProceedings{2017_EA_82,
author    = {Allison, Jesse and Ostrenko, Frederick and Cellucci, Vincent},
title     = {Diamonds in Dystopia},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2--3},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Diamonds in Dystopia is a body of work and web framework for creatively datamining large sources of text for mobile interaction. So far we have used it for live-streaming poetry performances at various locations such as SXSW Interactive and TEDx in addition to fine arts installations. As a performance it is a web driven app for incorporating improvisation into experiential storytelling. The audience acts as collaborator by sending word selections by tapping language on their mobiles to trigger reactions to send a distilled, improvisational stanza culled from a massive corpus of text to the poet on stage. The individual taps coming from the audience also trigger synthesized audio effects at varying pitches to create a musical experience as well as contributing to a visual projection of the poem and audience interactivity. Created by Vincent A. Cellucci (poet), Jesse Allison (Professor of Experimental Music), and Derick Ostrenko (Professor of Digital Art), the applications use natural language processing on text to generate an innovative media stage project. The app creators are interested in creative data mining and incorporating interactive media into performances that challenge people's perceptions and expectations for the mediums of music, digital art and design, and poetry.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Allison, Ostrenko, Cellucci - 2017 - Diamonds in Dystopia.pdf:pdf},
keywords  = {Other},
type      = {Performance},
}

@InProceedings{2017_25,
author    = {Roma, Gerard and Xambó, Anna and Freeman, Jason},
title     = {Loop-aware Audio Recording for the Web},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Music loops are audio recordings used as basic building blocks in many types of music. The use of pre-recorded loops facilitates engagement into music creation to users regardless of their background in music theory. Using online loop databases also affords simple collaboration and exchange. Hence, music loops are particularly attractive for web audio applications. However, traditional musical audio recording typically relies on complex DAW software. Recording loops usually requires consideration of musical meter and tempo, and withstanding metronome sounds. In this paper, we propose loop-aware audio recording as a use case for web audio technologies. Our approach supports hands-free, low-stress recording of music loops in web-enabled devices. The system is able to detect repetitions in an incoming audio stream. Based on this information, it segments and ranks the repeated fragments, presenting the list to the user. We provide an example implementation, and evaluate the use of the different MIR libraries available in the web audio platform for the proposed task.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Roma, Xamb{\'{o}}, Freeman - 2017 - Loop-aware Audio Recording for the Web.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/BhL3J5hcwNE?t=6508},
}

@InProceedings{2017_EA_44,
author    = {Volke, Stephanus and Bechtold, Bastian and Bitzer, Joerg},
title     = {HTML Web Audio Elements: Easy Interaction with Web Audio API Through HTML},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {7--8},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {The JavaScript Web Audio API has a powerful but low-level and complicated structure. Therefore, many JavaScript-based wrapper libraries exist, which are intended to simplify its usage. This paper presents a completely new approach, which translates the API into HTML Custom Elements and allows definition, usage and control of complex audio scarios using only normal HTML elements.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Volke, Bechtold, Bitzer - 2017 - HTML Web Audio Elements Easy Interaction with Web Audio API Through HTML.pdf:pdf},
type      = {Demo},
}

@InProceedings{2017_EA_61,
author    = {Wilmering, Thomas and Thalmann, Florian and Sandler, Mark B},
title     = {Towards a Framework for the Discovery of Collections of Live Music Recordings and Artefacts on the Semantic Web},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This paper introduces a platform for the representation and discovery of live music recordings and associated artefacts based on a dedicated data model. We demonstrate our technology by implementing a Web-based discovery tool for the Grateful Dead collection of the Internet Archive, a large collection of concert recordings annotated with editorial meta- data. We represent this information using a Linked Data model complemented with data aggregated from several additional Web resources discussing and describing these events. These data include descriptions and images of physical artefacts such as tickets, posters and fan photos, as well as other information, e.g. about location and weather. The system uses signal processing techniques for the analysis and alignment of the digital recordings. During the discovery, users can juxtapose and compare dierent recordings of a given concert, or different performances of a given song by interactively blending between them.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Wilmering, Thalmann, Sandler - 2017 - Towards a Framework for the Discovery of Collections of Live Music Recordings and Artefacts on the.pdf:pdf},
type      = {Poster},
}

@InProceedings{2017_EA_12,
author    = {Donovan, Michael and Seetharaman, Prem and Pardo, Bryan},
title     = {A Web Audio Node for the Fast Creation of Natural Language Interfaces for Audio Production},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Audio production involves the use of tools such as reverberators, compressors, and equalizers to transform raw audio into a state ready for public consumption. These tools are in wide use by both musicians and expert audio engineers for this purpose. The typical interfaces for these tools use low-level signal parameters as controls for the audio effect. These signal parameters often have unintuitive names such as “feedback” or “low-high” that have little meaning to many people. This makes them difficult to use and learn for many people. Such low-level interfaces are also common throughout audio production interfaces using the Web Audio API. Recent work in bridging the semantic gap between verbal descriptions of audio effects (e.g. “underwater”, “warm”, “bright”) and low-level signal parameters has resulted in provably better interfaces for a population of laypeople. In that work, a vocabulary of hundreds of descriptive terms was crowdsourced, along with their mappings to audio effects settings for rever- beration and equalization. In this paper, we present a Web Audio node that lets web developers leverage this vocabulary to easily create web-based audio effects tools that use natural language interfaces. Our Web Audio node and additional documentation can be accessed at https://interactiveaudiolab.github.io/audealize_api.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Donovan, Seetharaman, Pardo - 2017 - A Web Audio Node for the Fast Creation of Natural Language Interfaces for Audio Production.pdf:pdf},
type      = {Poster},
}

@InProceedings{2017_EA_58,
author    = {Takakura, Hiroyuki},
title     = {WebbyJam, a Web Tune Editor to Find Enjoyment},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2017},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This paper describes the functions, concept and technical points of WebbyJam ( http://www.webbyjam.com ).},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Takakura - 2017 - WebbyJam, a Web Tune Editor to Find Enjoyment Helpful to Find Enjoyment.pdf:pdf},
type      = {Artwork},
}

@InProceedings{2017_EA_54,
author    = {Thompson, Lucas and Cannam, Chris and Sandler, Mark},
title     = {Piper: Audio Feature Extraction in Browser and Mobile Applications},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2--3},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Piper is a protocol for audio analysis and feature extraction. We propose a data schema and API that can be used to support both remote audio feature extraction services and feature extractors loaded directly into a host application. We provide a means of using existing audio feature extractor implementations with this protocol. In this talk we demonstrate several use-cases for Piper, including an“audio notebook”mobile application using Piper modules to analyse recordings; a web service for remote feature extraction; and the refactoring of an existing desktop application, Sonic Visualiser, to communicate with a Piper service using a simple IPC mechanism.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Thompson, Cannam, Sandler - 2017 - Piper Audio Feature Extraction in Browser and Mobile Applications.pdf:pdf},
type      = {Talk},
url       = {https://youtu.be/OpUeyRRPpCo?t=1889},
}

@InProceedings{2017_EA_9,
author    = {Bechtold, Bastian and Volke, Stephanus and Bitzer, Joerg},
title     = {Web Technologies for Scientific Hearing Experiments and Teaching - An Overview},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Scientists of many audio-related fields need to verify their theories by conducting controlled experiments with human test subjects. The process of developing and conducting such experiments often poses non-trivial challenges to scientists and test subjects. Web technologies promise simple delivery of experiments as interactive websites, possible even on subjects' own computers. Similar benefits are possible for teaching science. While many tasks in hearing experiments and teaching are well-supported with current web-based tools, suuport for scientific data structures, signal processing operations and statistical data analysis methods is still incomplete in comparison with entrenched non-web tools. These shortcomings could easily be overcome with a few libraries, and would provide a great boon to scientists and educators.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Bechtold, Volke, Bitzer - 2017 - Web Technologies for Scientific Hearing Experiments and Teaching - An Overview.pdf:pdf},
type      = {Poster},
}

@InProceedings{2017_EA_31,
author    = {Mckegg, Matt and Schuhfuss, Martin and Pietrusky, Tim},
title     = {Live JS (performance)},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2131},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {A completely live electronic music performance with visuals and lighting entirely powered by JavaScript. Following on from my submitted talk "Is Web Audio ready for the stage/dancefloor", I along with other members of Live JS (http:// livejs.network) would like to demonstrate that it INDEED is ready! At JSConf.asia, we were given the opportunity to take over the entire venue (including their massive lighting rig) and broadcast our pirate JavaScript signal to everyone for the after party! We'll be doing something similar at JSConf.eu in May (if all goes to plan). Here is a video of our JSConf.asia talk (30mins) and full improvised performance: https://youtu.be/s3fsRnFfyuo?t=2131 Matt, Martin and Tim (possibly more of us, depending on who can come) will be dropping Web Audio powered samples, synthesisers, and looping using multiple midi controllers. We'll also be using JavaScript to drive a massive LED wall, realtime visuals and (if possible) moving head lighting via DMX.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mckegg, Schuhfuss, Pietrusky - 2017 - Live JS (performance).pdf:pdf},
type      = {Performance},
}

@InProceedings{2017_59,
author    = {Park, TH and Yoo, M},
title     = {Practicable Soundmapping: JavaScript enabled Edge Compute},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {In this paper, we report on developments of the Citygram, a comprehensive sensor network platform for capturing, streaming, analyzing, mapping, visualizing, and providing easy access to spatiotemporal soundscape data. Launched in 2011, Citygram's recent strategic decision has resulted in system redesign to enable migration from a codebase built on multiple computer languages to a cross-platform single JavaScript codebase. Citygram now runs on V8 engines including standard web-browsers and in the node.js environment. The Citygram sensor network significantly alleviates problems concerning soundmapping complexities including operating system limitations, hardware dependency, software update and dissemination issues, data access mechanism, data visualization, and cost. This strategy has made practicable a key design philosophy for soundmapping: rapid sensor network growth for capturing spatiotemporally granular soundscapes. We summarize research and development for the following Citygram modules (1) cost-effective sensor module allowing high-value data transmission through edge compute paradigms, (2) machine learning module focusing on environmental sound classification, and (3) visualization and data access prototypes.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Park, Yoo - 2017 - Practicable Soundmapping JavaScript enabled Edge Compute.pdf:pdf},
type      = {Paper},
}

@InProceedings{2017_EA_10,
author    = {Parkinson, Adam and Zbyszynski, Michael and Bernardo, Francisco},
title     = {Demonstrating Interactive Machine Learning Tools for Rapid Prototyping of Gestural Instruments in the Browser},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {1--2},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {These demonstrations will allow visitors to prototype gestural, interactive musical instruments in the browser. Different browser based synthesisers can be controlled by either a Leap Motion sensor or a Myo armband. The visitor will be able to use an interactive machine learning toolkit to quickly and iteratively explore different interaction possibilities. The demonstrations show how interactive, browser-based machine learning tools can be used to rapidly prototype gestural controllers for audio. These demonstrations showcase RapidLib, a browser based machine learning library developed through the RAPID-MIX project.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Parkinson, Zbyszynski, Bernardo - 2017 - Demonstrating Interactive Machine Learning Tools for Rapid Prototyping of Gestural Instruments.pdf:pdf},
keywords  = {Article},
type      = {Demo},
}

@InProceedings{2017_EA_27,
author    = {Buffa, Michel and Lebrun, Jerome},
title     = {Web Audio Guitar Tube Amplifier vs Native Simulations},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2--4},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {We propose to present a tube guitar amplifier simulation we've been designing using the Web Audio API with the aim to faithfully reproduce the main parts of the Marshall JCM 800 amplifier schematics. Each stage of the real amp has been recreated (preamp, tone stack, reverb, power amp and speaker simulation). We've also added an extra multiband EQ. This “classic rock” amp simulation we've been building has been used in real gigs and can be favorably compared with some native amp simulation both in terms of latency, sound quality, dynamics and comfort of the guitar play. The amp is open source1 and can be tested online2, even without a real guitar plugged-in. It comes with an audio player, dry guitar samples and a wave generator that can be used as inputs. Figure 1 shows the current GUI, with some optional frequency analyzers and oscilloscopes that we've been using to probe the signal at different stages of the simulation. One purpose was to evaluate the limits of the Web Audio API and see if it was possible to design a web-based guitar amp simulator that could compete with native simulations.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Buffa, Lebrun - 2017 - Web Audio Guitar Tube Amplifier vs Native Simulations.pdf:pdf},
type      = {Demo},
}

@InProceedings{2017_EA_67,
author    = {Becker, François and Bernard, Benjamin and Carron, Clément},
title     = {ARCADE 3D-audio codec: an implementation for the web},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This poster introduces the implementation of the ARCADE 3D-audio codec for web browsers. ARCADE can embed a full 3D audio scene in a simple stereo-compatible audio stream that can be further compressed with standard lossy compression schemes, aired to analog or digital radio receivers or even stored on analog supports. An ARCADE-encoded stream can be decoded to any 2D or 3D-audio rendering format, for instance using Vector-Based Amplitude Panning (VBAP), Higher Order Ambisonics (HOA), or personalized binaural with headtracking. ARCADE adapts seamlessly to the audio industry needs, from storage to production, distribution/delivery, and rendering. It finds uses in Virtual or Augmented Reality (VR/AR), movies, gaming, music, telepresence & teleconferencing. We present a JavaScript (JS) and Web Audio API implementation of the ARCADE decoder, which was originally written in C++11, along with technical details of the porting operations. Live demos of 3D-audio content transmission, decoding and dynamic binaural rendering will be given during the poster session.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Becker, Bernard, Carron - 2017 - ARCADE 3D-audio codec an implementation for the web.pdf:pdf},
type      = {Demo},
}

@InProceedings{2017_39,
author    = {Matuszewski, Benjamin and Schnell, Norbert},
title     = {LFO – A Graph-based Modular Approach to the Processing of Data Streams},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This paper introduces lfo — for Low Frequency Operators — a graph-based Javascript (ES2015) API for online and offline processing (i.e. analysis and transformation) of data streams such as audio and motion sensor data. The library is open-source and entirely based on web standards. The project aims at creating an ecosystem consisting of platform-independent stream operator modules such as filters and extractors as well as platform-specific source and sink modules such as audio i/o, motion sensor inputs, and file access. The modular approach of the API allows for using the library in virtually any Javascript environment. A first set of operators as well as basic source and sink modules for web browsers and Node.js are included in the distribution of the library. The paper introduces the underlying concepts, describes the implementation of the API, and reports on benchmarks of a set of operators. It concludes with the presentation of a set of example applications.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Matuszewski, Schnell - 2017 - LFO – A Graph-based Modular Approach to the Processing of Data Streams.pdf:pdf},
keywords  = {digital signal processing,html5,web audio api},
type      = {Paper},
url       = {https://youtu.be/mo6VKewheGU?t=3688},
}

@InProceedings{2017_EA_62,
author    = {Thalmann, Florian and Ewert, Sebastian and Wiggins, Geraint and Sandler, Mark B},
title     = {Exploring Musical Expression on the Web: Deforming, Exaggerating, and Blending Decomposed Recordings},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {We introduce a prototype of an educational web application for comparative performance analysis based on source separation and object-based audio techniques. The underlying system decomposes recordings of classical music performances into note events using score-informed source separation and represents the decomposed material using semantic web technologies. In a visual and interactive way, users can explore individual performances by highlighting specific musical aspects directly within the audio and by altering the temporal characteristics to obtain versions in which the micro-timing is exaggerated or suppressed. Multiple performances of the same work can be compared by juxtaposing and blending between the corresponding recordings. Finally, by adjusting the timing of events, users can generate intermediates of multiple performances to investigate their commonalities and differences.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Thalmann et al. - 2017 - Exploring Musical Expression on the Web Deforming, Exaggerating, and Blending Decomposed Recordings.pdf:pdf},
type      = {Poster},
}

@InProceedings{2017_EA_17,
author    = {Meléndez-Catalán, Blai and Molina, Emilio and Gómez, Emilia},
title     = {BAT: An open-source, web-based audio events annotation tool},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {3--6},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {In this paper we present BAT (BMAT Annotation Tool), an open-source, web-based tool for the manual annotation of events in audio recordings developed at BMAT (Barcelona Music and Audio Technologies1). The main feature of the tool is that it provides an easy way to annotate the salience of simultaneous sound sources. Additionally, it allows to define multiple ontologies to adapt to multiple tasks and offers the possibility to cross-annotate audio data. Moreover, it is easy to install and deploy on servers. We carry out an evaluation where 3 annotators use BAT to annotate a small dataset composed of broadcast media recordings. The results of the experiments show that BAT offers fast annotation mechanisms and a method to assign salience that produces high agreement among annotators.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mel{\'{e}}ndez-Catal{\'{a}}n, Molina, G{\'{o}}mez - 2017 - BAT An open-source, web-based audio events annotation tool.pdf:pdf},
keywords  = {annotation tool,audio events,open-source,web-based},
type      = {Poster},
}

@InProceedings{2017_72,
author    = {Roberts, Charles},
title     = {Strategies for Per-Sample Processing of Audio Graphs in the Browser},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
volume    = {7},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Due to current browser limitations, most synthesis in the browser is currently performed using the block-rate nodes included in the WebAudio API. However, block-rate processing of audio graphs precludes many types of synthesis in addition to limiting both the accuracy and flexibility of scheduling. We describe alternative strategies for performing efficient, per-sample processing of audio graphs in the browser using the ScriptProcessor node, affording synthesis techniques that are not commonly found in existing JavaScript audio libraries. We introduce a new library, Genish.js, that provides unit generators for common low-level synthesis tasks and acts as a compiler for signal processing functions; this library is a loose port of the Gen framework for Max/MSP. We used Genish.js to update a higher-level library for audio programming, Gibberish.js, realizing improvements to both efficiency and audio quality. Preliminary benchmarks comparing the performance of Genish.js audio graphs to equivalent graphs made with the WebAudio API show promising results.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Roberts - 2017 - Strategies for Per-Sample Processing of Audio Graphs in the Browser.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/mo6VKewheGU?t=2352},
}

@InProceedings{2017_EA_13,
author    = {Stolfi, Ariane and Barthet, Mathieu and Goródscy, Fábio},
title     = {Open Band: Audiotype},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Open Band is a collective performance, that deals with a contradiction of the social media, that is the apartness of the individual on their devices social media, to propose a collective sound intervention, were a conductor interacts with the audience through an anonymous chat interface that converts text into sound messages. In this version, we are working only with web audio synthesis, based on an idea of audio typography.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Stolfi, Barthet, Gor{\'{o}}dscy - 2017 - Open Band Audiotype.pdf:pdf},
type      = {Performance},
}

@InProceedings{2017_EA_37,
author    = {Jillings, Nicholas and Stables, Ryan},
title     = {An Intelligent audio workstation in the browser},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Music production is a complex process requiring skill and time to undertake. The industry has undergone a digital revolution, but unlike other industries the process has not changed. However, intelligent systems, using the semantic web and signal processing, can reduce this complexity by making certain decisions for the user with minimal interaction, saving both time and investment on the engineers' part. This paper will outline an intelligent Digital Audio Workstation (DAW) designed for use in the browser. It outlines the architecture of the DAW with its audio engine (built on the Web Audio API), using AngularJS for the user interface and a relational database.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Jillings, Stables - 2017 - An Intelligent audio workstation in the browser.pdf:pdf},
keywords  = {Article},
type      = {Poster},
}

@InProceedings{2017_EA_79,
author    = {Vasallo, Thomas and Benito, Adan L},
title     = {Real Time Synthesized Sound Effects Web Service},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {Sound effects are employed in the post-production process in order to create tension, atmosphere, and emotion, as well as add focus to desired aspects of a scene. Traditionally sound designers are required to either source these sounds from commercially available libraries or to record audio themselves. Additionally, sound designers are usually required to manually manipulate these sources in order to accurately sonify the scene. This whole process requires time, planning and effort from the sound designers. While traditional synthesis techniques have been adapted to be incorporated into this process, the synthesisers employed are almost always not designed for this particular purpose. The RTSFX (Real Time Sound Effect Synthesis) web-based platform offers a range of synthesis models tailored to recreate a spectrum of sound effects that may be used for this task. An exposed set of parameters allows for the fine tuning and manipulation of a particular sound object to match the desired characteristics. A basic selection of post processing options is found within the platform to allow for a self-contained sound design process. A browser based platform has been created on which the sounds are generated in real time. A client-side architecture has been employed, allowing for a more flexible workflow for the sound designers. This approach makes it easily accessible to the user, while simultaneously not requiring any permanent local memory allocation. The browser-based aspect means that the platform is not limited by server availability, while also providing a low latency experience. RTSFX relies on the standardised Web Audio API in order to establish a framework for synthesising effects. Different models can be generated using a mixture of Web-Audio API nodes, customised Javascript processors or Pure Data patches through the use of WebPD or Enzien Audio Heavy. A number of different approaches are used in the model design process. Ranging from accurate representations of physical phenomena, to more perceptually informed qualitative methods. RTSFX offers a centralised set of elements which may be utilized to build these designs. The platform is an ever growing source of synthesis models with the scope of incorporating more complex techniques which involve the analysis of audio sources in order to generate models.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Vasallo, Benito - 2017 - Real Time Synthesized Sound Effects Web Service.pdf:pdf},
type      = {Demo},
}

@InProceedings{2017_11,
author    = {Stolfi, Ariane and Barthet, Mathieu and Goródscy, Fábio and Deusany, Antonio and Iazzetta, Fernando},
title     = {Open band: Audience Creative Participation Using Web Audio Synthesis},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This work investigates a web-based open environment enabling collaborative music experiences. We propose an artifact, Open Band, which enables collective sound dialogues in a web "agora", blurring the limits between audience and performers. The systems relies on a multi-user chat system where textual inputs are translated to sounds. We depart from individual music playing experiences in favor of creative participation in networked music making. A previous implementation associated typed letters to precomposed samples. We present and discuss in this paper a novel instance of our system which operates using Web Audio synthesis.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Stolfi et al. - 2017 - Open band Audience Creative Participation Using Web Audio Synthesis.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/BhL3J5hcwNE?t=10167},
}

@InProceedings{2017_EA_30,
author    = {Mckegg, Matt},
title     = {Is Web Audio ready for the Stage / Dancefloor?},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {2017},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {The web platform now appears to have all the pieces needed to drive a full live electronic music setup. We can connect to keyboards and controllers using MIDI, synthesise any sound and process live audio. We can also leverage the vast UI toolkit that is HTML and CSS. We can build a DAW (digital audio workstation), but is it solid enough to use realtime on stage? Could we play an entire show with just JavaScript and a few midi controllers? I have been experimenting with this idea for the last couple of years. Using my own software (with a user interface designed specifically for playing live), I've now played a bunch of semi-improvised shows and learnt a whole lot along the way. From user interface design to squeezing as much performance out of Web Audio and JavaScript, I have a lot to share! For reference, here's a selection of videos featuring me playing live electronic music entirely with Web Audio and JavaScript: https:// www.youtube.com/user/mmckegg/videos WEB},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mckegg - 2017 - Is Web Audio ready for the Stage Dancefloor.pdf:pdf},
type      = {Talk},
url       = {https://youtu.be/OpUeyRRPpCo?t=3715},
}

@InProceedings{2017_63,
author    = {Poirier-Quinot, David and Matuszewski, Benjamin and Schnell, Norbert and Warusfel, Olivier},
title     = {Nü Soundworks: Using spectators smartphones as a distributed network of speakers and sensors during live performances.},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {0--5},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {This paper presents the Nü framework. The objective of the framework is to provide composers with a tool to control web-based audio processes on spectators smartphones during live performances. Connecting their devices to a webpage broadcasted by the performer's laptop, spectators become part of the composition: from simple sound sources to active musicians. From a Max based interface, the performer can then control the behaviours of conceptual units, referred to as Nü modules, designed for live composition (distributed room reverb, granular synthesis, etc.). Each module is composed of a pair of JavaScript classes – one for the client, another for the server – relying on the Web Audio API for audio processing, and OSC messages for parameters control. Nü is an open source project based on the Soundworks framework.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Poirier-Quinot et al. - 2017 - N{\"{u}} Soundworks Using spectators smartphones as a distributed network of speakers and sensors during live.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/BhL3J5hcwNE?t=5488},
}

@InProceedings{2017_EA_49,
author    = {Schnell, Norbert and Matuszewski, Benjamin},
title     = {Playing (with) Mobile Devices and Web Standards ... Together},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2017},
editor    = {Thalmann, Florian and Ewert, Sebastian},
series    = {WAC '17},
pages     = {0--1},
address   = {London},
month     = aug,
publisher = {Queen Mary University of London},
abstract  = {In this talk, we present an overview of the experiences we conducted with developers, artists, pedagogues, and many different audiences over the past three years in the framework of the CoSiMa research project. The hypothesis of our research was that people would more or less spontaneously take their mobile device out of their pocket to join others around them in making music together. Our methodology basically consisted in trying everything we could and taking on any collaboration that fitted this hypothesis while carefully observing various design aspects. We have focussed in this work on the exploration of affordances of mobile devices and web standards which we consider to be strong ecological factors in the development of communication, entertainment, and poetry. The talk includes a panorama of the applications and scenarios (i.e. participative concerts, installations, and work-shops) we have developed over the past three years as well as an overview of the most important findings we have been able to formulate so far. The presentation concludes with a collective improvisation in which we invite the audience to participate.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Schnell, Matuszewski - 2017 - Playing ( with ) Mobile Devices and Web Standards ... Together.pdf:pdf},
type      = {Talk},
url       = {https://youtu.be/HjBqB3g8y2A?t=4732},
}